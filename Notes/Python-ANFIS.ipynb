{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python code for an ANFIS\n",
    "\n",
    "By: Jianshan Zhou\n",
    "\n",
    "Email: jianshanzhou@foxmail.com\n",
    "\n",
    "Webï¼šhttps://github.com/JianshanZhou/Computing-Intelligence-Course.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Copyright (C) Tue Nov 29 18:25:39 2016  Jianshan Zhou\n",
    "Contact: zhoujianshan@buaa.edu.cn\tjianshanzhou@foxmail.com\n",
    "Website: <https://github.com/JianshanZhou>\n",
    "\n",
    "This program is free software: you can redistribute\n",
    " it and/or modify it under the terms of\n",
    " the GNU General Public License as published\n",
    " by the Free Software Foundation,\n",
    " either version 3 of the License,\n",
    " or (at your option) any later version.\n",
    " \n",
    "This program is distributed in the hope that it will be useful,\n",
    " but WITHOUT ANY WARRANTY;\n",
    " without even the implied warranty of MERCHANTABILITY\n",
    " or FITNESS FOR A PARTICULAR PURPOSE.\n",
    " See the GNU General Public License for more details.\n",
    " You should have received a copy of the GNU General Public License\n",
    " along with this program.\n",
    " If not, see <http://www.gnu.org/licenses/>.\n",
    " \n",
    "This module processes the original data.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_test_data1():\n",
    "    data = np.loadtxt(\"sys_data.csv\")\n",
    "    sample_num = 500\n",
    "    training_data = []\n",
    "    all_data = []\n",
    "    for i in range(sample_num):\n",
    "        x1 = data[100+i]\n",
    "        x2 = data[106+i]\n",
    "        x3 = data[112+i]\n",
    "        x4 = data[118+i]\n",
    "        x = np.array([x1,x2,x3,x4])\n",
    "        y = np.array([data[124+i]])\n",
    "        training_data.append((x,y))\n",
    "        all_data.append((x,y))\n",
    "        \n",
    "    validation_data = []\n",
    "    for i in range(sample_num):\n",
    "        x1 = data[600+i]\n",
    "        x2 = data[606+i]\n",
    "        x3 = data[612+i]\n",
    "        x4 = data[618+i]\n",
    "        x = np.array([x1,x2,x3,x4])\n",
    "        y = np.array([data[624+i]])\n",
    "        validation_data.append((x,y))\n",
    "        all_data.append((x,y))\n",
    "\n",
    "    return training_data,validation_data,all_data\n",
    "\n",
    "\n",
    "def _test1():\n",
    "    data = np.loadtxt(\"sys_data.csv\")\n",
    "    print data.shape\n",
    "    labelfont = {\"family\":\"serif\",\"size\":20}\n",
    "    plt.figure(0,figsize=(9,8))\n",
    "    plt.plot(data,'-r',lw=8.0, label=\"Fuel comsuption\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(prop={\"size\":labelfont[\"size\"],\"family\":\"serif\"})  \n",
    "    xlabelstr = \"Epoch $k$\"\n",
    "    ylabelstr = \"Fuel level\"\n",
    "    plt.xlabel(xlabelstr,fontdict=labelfont)\n",
    "    plt.ylabel(ylabelstr,fontdict=labelfont)\n",
    "    plt.xticks(fontsize=labelfont[\"size\"],fontname=labelfont[\"family\"])\n",
    "    plt.yticks(fontsize=labelfont[\"size\"],\n",
    "               family=labelfont[\"family\"])\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_test_data1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Copyright (C) Sat Nov 26 17:44:07 2016  Jianshan Zhou\n",
    "Contact: zhoujianshan@buaa.edu.cn\tjianshanzhou@foxmail.com\n",
    "Website: <https://github.com/JianshanZhou>\n",
    " \n",
    "This module implements a Least Square Estimator (LSE) for solving\n",
    "argmin:||AX-B||^2 given the data (A,B).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def LSE(A,B,X):\n",
    "    \"\"\"Perform a sequential algorithm to solve the LSE of\n",
    "    ||AX-B||^2, which can be easily implemented in an online learning fashion.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A: a 2-D array, shape (P,M)\n",
    "    B: a 1-D array, shape (P,)\n",
    "    \n",
    "    Returns\n",
    "    X: a 2-D array, shape (M,1)    \n",
    "    \"\"\"\n",
    "    (P,M) = A.shape\n",
    "    # initialize Si\n",
    "    gamma = 1e8\n",
    "    S = gamma*np.identity(M,dtype=float)\n",
    "    for i in range(P):\n",
    "        # the i-th row vector of A        \n",
    "        a_transpose = A[i,::]\n",
    "        # keep it as a 2-D array\n",
    "        a_transpose = a_transpose.reshape((1,len(a_transpose)))\n",
    "        # the i-th element of B\n",
    "        b_transpose = B[i]\n",
    "        den = 1.0+np.dot(a_transpose,np.dot(S,a_transpose.T))\n",
    "        num = np.dot(np.dot(S,a_transpose.T),np.dot(a_transpose,S))\n",
    "        S = S - num/den\n",
    "        X = X + np.dot(S,a_transpose.T)*(b_transpose-np.dot(a_transpose,X))\n",
    "    return X\n",
    "\n",
    "def LSE2(a, X, Y, solution):\n",
    "    \"\"\"Similar to LSE but receiving different inputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a: a 1-D array, shape (N,)\n",
    "    X: a 2-D array, shape (P,n)\n",
    "    Y: a 1-D array, shape (P,)\n",
    "    solution: a 2-D array, shape (M,1) where M=N*n\n",
    "    \n",
    "    Returns\n",
    "    solution: a 2-D array, shape (M,1) where M=N*n\n",
    "    \"\"\"\n",
    "    N = len(a)\n",
    "    (P,n) = X.shape\n",
    "    M = N*n\n",
    "    \n",
    "    # initialize Si\n",
    "    gamma = 1e8\n",
    "    S = gamma*np.identity(M,dtype=float)\n",
    "    for i in range(P):\n",
    "        # the i-th row vector of A        \n",
    "        a_transpose = np.hstack((ak*X[i,::] for ak in a))\n",
    "        # keep it as a 2-D array\n",
    "        a_transpose = a_transpose.reshape((1,len(a_transpose)))\n",
    "        # the i-th element of B\n",
    "        b_transpose = Y[i]\n",
    "        den = 1.0+np.dot(a_transpose,np.dot(S,a_transpose.T))\n",
    "        num = np.dot(np.dot(S,a_transpose.T),np.dot(a_transpose,S))\n",
    "        S = S - num/den\n",
    "        solution = solution \\\n",
    "        + np.dot(S,a_transpose.T)*(b_transpose-np.dot(a_transpose,solution))\n",
    "    return solution\n",
    "\n",
    "\n",
    "def LSE3(A, X, Y, solution):\n",
    "    \"\"\"Similar to LSE but receiving different inputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A: a list of lists, shape (P,N)\n",
    "    X: a 2-D array, shape (P,n)\n",
    "    Y: a 1-D array, shape (P,)\n",
    "    solution: a 2-D array, shape (M,1) where M=N*n\n",
    "    \n",
    "    Returns\n",
    "    solution: a 2-D array, shape (M,1) where M=N*n\n",
    "    \"\"\"\n",
    "    N = len(A[0])\n",
    "    (P,n) = X.shape\n",
    "    M = N*n\n",
    "    \n",
    "    # initialize Si\n",
    "    gamma = 1e8\n",
    "    S = gamma*np.identity(M,dtype=float)\n",
    "    for i in range(P):\n",
    "        # the i-th row vector of the coef. matrix\n",
    "        a = A[i]       \n",
    "        a_transpose = np.hstack((ak*X[i,::] for ak in a))\n",
    "        # keep it as a 2-D array\n",
    "        a_transpose = a_transpose.reshape((1,len(a_transpose)))\n",
    "        # the i-th element of B\n",
    "        b_transpose = Y[i]\n",
    "        den = 1.0+np.dot(a_transpose,np.dot(S,a_transpose.T))\n",
    "        num = np.dot(np.dot(S,a_transpose.T),np.dot(a_transpose,S))\n",
    "        S = S - num/den\n",
    "        solution = solution \\\n",
    "        + np.dot(S,a_transpose.T)*(b_transpose-np.dot(a_transpose,solution))\n",
    "    return solution\n",
    "\n",
    "\n",
    "def data_format(a,X,Y):\n",
    "    \"\"\"This function is used to rearrange the data representation\n",
    "    into a compact matrix form.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a: a 1-D array, shape (N,)\n",
    "    X: a 2-D array, shape (P,n)\n",
    "    Y: a 1-D array, shape (P,)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    A: a 2-D array, shape (P,N*n)\n",
    "    \n",
    "    \"\"\"\n",
    "    (P,n) = X.shape\n",
    "    x = X[0,::] # a 1-D array with shape (1,n)\n",
    "    A = [ak*x for ak in a]\n",
    "    A = np.asarray(A)\n",
    "    A = A.reshape(1,-1)\n",
    "    for i in range(P):\n",
    "        if i !=0:\n",
    "            y = np.asarray([ak*X[i,::] for ak in a])\n",
    "            y = y.reshape(1,-1)\n",
    "            A = np.vstack((A,y))\n",
    "    return A\n",
    "    \n",
    "\n",
    "def vectorize(W):\n",
    "    \"\"\"This function transforms a 2-D array, i.e., a matrix, \n",
    "    into a column vector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W: a 2-D array, shape (M,N)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    w: a 2-array, shape (M*N,1)\n",
    "    \"\"\"\n",
    "    return W.reshape(-1,1)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # perform a test on this LSE algorithm\n",
    "    x = np.array([0, 1, 2, 3])\n",
    "    y = np.array([-1, 0.2, 0.9, 2.1])\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    print A\n",
    "    m = np.linalg.lstsq(A,y)[0]\n",
    "    print m\n",
    "    \n",
    "    solution = np.zeros((2,1),dtype=float)\n",
    "    s = LSE(A,y,solution)\n",
    "    print s\n",
    "    \n",
    "    print np.linalg.norm(s-m.reshape(s.shape))\n",
    "    \n",
    "    a = np.array([1.0])\n",
    "    print a\n",
    "    print A\n",
    "    B = data_format(a,A,y)\n",
    "    print B\n",
    "    \n",
    "    print \"-----------\"\n",
    "    print vectorize(A)\n",
    "    print \"-------------\"\n",
    "    print LSE2(a, A, y, solution)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## anfis_mfs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 30 09:24:46 2016\n",
    "\n",
    "@author: zhoujianshan\n",
    "\n",
    "This module provides some membership functions as well as their corresponding\n",
    "partial derivatives of every parameters.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import copy \n",
    "\n",
    "def gbellmf(x,params):\n",
    "    \"\"\"Generalized bell-shaped membership function:\n",
    "    GBELLMF(X, [A, B, C]) = 1./((1+ABS((X-C)/A))^(2*B));\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: a real number\n",
    "    params: is a list-like or 1-D array-like vector containing\n",
    "    parameters of this function.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    output: a real number, also\n",
    "    \"\"\"\n",
    "    if len(params) != 3:\n",
    "        raise ValueError(\"Something is wrong with the length of the \\\n",
    "        parameter list!\")\n",
    "    a = params[0]\n",
    "    b = params[1]\n",
    "    c = params[2]\n",
    "    if np.abs(a)<=1e-10:\n",
    "        print a\n",
    "        raise(\"Warning: The parameter a of GBELLMF is approximatively zero!!\")\n",
    "    tmp = ((x-c)/a)**2\n",
    "    if (tmp==0) and (b==0):\n",
    "        output = 0.5\n",
    "    elif (tmp==0) and (b<0):\n",
    "        output = 0\n",
    "    else:\n",
    "        tmp = tmp**b\n",
    "        output = 1.0/(1+tmp)\n",
    "    return output\n",
    "        \n",
    "\n",
    "def dgbellmf(x,params):\n",
    "    \"\"\"Numerically calculate the partial derivatives of the mf parameters.\n",
    "    Note that here the mathematical definition of the derivative is used to\n",
    "    numerically approximate the partial derivative:\n",
    "    df(x)/dx = (f(x+EPSILON)-f(x-EPSILON))/(2*EPSILON)\n",
    "    According to UFLDL Tutorial: http://deeplearning.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n",
    "    EPSILON = 1e-4\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: a real number\n",
    "    params: is a list-like or 1-D array-like vector containing\n",
    "    parameters of this function.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dParams: a 1-D array, shape (len(params),)\n",
    "    \"\"\"\n",
    "    if len(params) != 3:\n",
    "        raise ValueError(\"Something is wrong with the length of the \\\n",
    "        parameter list!\")\n",
    "    EPSILON = 1e-4\n",
    "    dParams = np.zeros((len(params),))\n",
    "    for i in range(len(params)):\n",
    "        params1 = copy.deepcopy(params)#params should not be changed!!!!!!!\n",
    "        params2 = copy.deepcopy(params)\n",
    "        params1[i] = params1[i]+EPSILON\n",
    "        params2[i] = params2[i]-EPSILON\n",
    "        dParams[i] = (gbellmf(x,params1)-gbellmf(x,params2))/(2*EPSILON)\n",
    "    return dParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## anfis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Copyright (C) Sun Nov 27 01:54:10 2016  Jianshan Zhou\n",
    "\n",
    "Contact: zhoujianshan@buaa.edu.cn\tjianshanzhou@foxmail.com\n",
    "\n",
    "Website: <https://github.com/JianshanZhou>\n",
    "\n",
    " \n",
    "This module builds the Adaptive Network-based Fuzzy \n",
    "Inference System (ANFIS).\n",
    "\n",
    "References\n",
    "-----------\n",
    "Jang, J. S. R. (1993). Anfis: adaptive-network-based fuzzy inference system.\n",
    " IEEE Transactions on Systems Man & Cybernetics, 23(3), 665-685.\n",
    "\"\"\"\n",
    "# The standard modules\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# The third party modules\n",
    "import numpy as np\n",
    "\n",
    "# The supplementary modules\n",
    "import lse as lse\n",
    "from anfis_mfs import gbellmf, dgbellmf\n",
    "\n",
    "\n",
    "# The objective function class\n",
    "class QuadraticC(object):\n",
    "    @staticmethod\n",
    "    def C(a,y):\n",
    "        \"\"\"The cost function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        a: the output of the last layer of the ANFIS\n",
    "        y: the desired output\n",
    "        a and y should be in the same size, and in 2-D array form\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        the quadratic cost, a positive number\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "        \n",
    "    @staticmethod\n",
    "    def delta(a,y):\n",
    "        \"\"\"Calculate the error rate dC/da for a given training data for each node\n",
    "        output.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        a: the output of the last layer of the ANFIS\n",
    "        y: the desired output\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        the error rate in size of a, 2-D array, shape(m,1)\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "# Main ANFIS class\n",
    "class ANFIS(object):\n",
    "    \n",
    "    def __init__(self, in_out_num, \n",
    "                 mf_num_vector=None, \n",
    "                 cost=QuadraticC):\n",
    "        \"\"\"The initial vector of numbers of MF associated with each input.\n",
    "        In this stage, I consider a general multiple-input-multiple-output adaptive\n",
    "        network, i.e., a MIMO ANFIS.\n",
    "        \"\"\"\n",
    "        if (len(in_out_num) !=2):\n",
    "            raise ValueError(\"Something is wrong with the numbers of inputs\\\n",
    "            and outputs!\")\n",
    "            \n",
    "        if mf_num_vector:\n",
    "            self.mf_num_vector = mf_num_vector\n",
    "        else:\n",
    "            self.mf_num_vector = [2 for i in range(in_out_num[0])]\n",
    "            \n",
    "        self.cost = cost\n",
    "        self.size = (in_out_num[0],\n",
    "                     np.sum(self.mf_num_vector),\n",
    "                     np.prod(self.mf_num_vector),\n",
    "                     np.prod(self.mf_num_vector),\n",
    "                     np.prod(self.mf_num_vector)*in_out_num[1],\n",
    "                     in_out_num[1]) # there are totally 6 layers in ANFIS\n",
    "        print \"A %d-%d-%d-%d-%d-%d ANFIS is initialized!\"%self.size\n",
    "\n",
    "    def initialization(self, training_data):\n",
    "        \"\"\"Initialize the premise and the consequent parameters of the ANFIS.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data: a list of tuples each denoting sample pair (x,y)\n",
    "        len(traning_data) = p\n",
    "        x is a 1-D array, shape (n,), and y is also a 1-D array, shape(m,)\n",
    "        where P is the total sample number, n the input number and m the output number.\n",
    "        \"\"\"\n",
    "        premiseParam, consequentParam = generateFIS(training_data, self.mf_num_vector)\n",
    "        self.premiseParam = premiseParam\n",
    "        self.consequentParam = consequentParam\n",
    "        for i in range(self.size[0]):\n",
    "            print \"The {0}-th input has {1} membership functions each with {2} premise \\\n",
    "            parameters\".format(i+1,len(self.premiseParam[i]),len(self.premiseParam[i][0]))\n",
    "        # Generate the mapped indices\n",
    "        self.indices_2nd_layer = index_of_2ndLayer(self.mf_num_vector)\n",
    "\n",
    "    def activations_3rd_layer(self, x):\n",
    "        \"\"\"Given a 2-D array and evaluate the outputs of the 3rd layer in the ANFIS.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a 2-D array, shape(n,1)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        activation1: a list of lists each containing the membership function \n",
    "        outputs of an input\n",
    "        \n",
    "        activation2: a list, shape (M,)\n",
    "        \n",
    "        activation3: a list, length N\n",
    "        \"\"\"\n",
    "        # Ensure 2-D\n",
    "        if len(x.shape)<2:\n",
    "            x = x.reshape(-1,1)\n",
    "        \n",
    "        # the activations of the 1st layer, 2-D\n",
    "        activation1 = [[gbellmf(x[i,0],self.premiseParam[i][ji]) for ji \\\n",
    "        in range(self.mf_num_vector[i])] for i in range(self.size[0])]\n",
    "                \n",
    "        # the activations of the 2nd layer, 1-D\n",
    "        activation2 = []\n",
    "        for ind in self.indices_2nd_layer:\n",
    "            a2_tmp = np.prod([activation1[i][ind[i]] for i in range(len(ind))])\n",
    "            activation2.append(a2_tmp)\n",
    "        \n",
    "        # the activations of the 3rd layer, 1-D\n",
    "        sum_a2 = np.sum(np.exp(activation2))\n",
    "        activation3 = [np.exp(a2_p)/sum_a2 for a2_p in activation2] # a list\n",
    "            \n",
    "        return activation1, activation2, activation3\n",
    "\n",
    "    def forwardpass(self, x):\n",
    "        \"\"\"Given a 2-D array and infere the outputs of each layer in by using the ANFIS.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a 2-D array, shape(n,1)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        activation1: a list of lists each containing the membership function \n",
    "        outputs of an input\n",
    "        \n",
    "        activation2: a list, shape (M,)\n",
    "        \n",
    "        activation3: a list, shape (N,) where N=m1*m2*...*mn, the total rules number\n",
    "        \n",
    "        activation4: the activations of the 4th layer, 2-D list, i.e., a list of lists\n",
    "        each corresponding to an output channel, shape (m, N)\n",
    "        \n",
    "        activation5: a 2-D array containing m outputs, shape (m,1)\n",
    "        \"\"\"\n",
    "        activation1, activation2, activation3 = self.activations_3rd_layer(x)\n",
    "        X = np.vstack((np.array([[1.]]),x)) # X shape (n+1,1)\n",
    "        # the activations of the 4th layer, 2-D\n",
    "        activation4 = []\n",
    "        for l in range(self.size[-1]):\n",
    "            # for the l-th output\n",
    "            a4_tmp = []\n",
    "            for q in range(len(activation3)):\n",
    "                consequent_params = self.consequentParam[l][q,::]\n",
    "                # Ensure 2-D\n",
    "                val = (np.dot(consequent_params.reshape(1,-1),X)) #2-D\n",
    "                a4_tmp.append(activation3[q]*val[0,0])\n",
    "            activation4.append(copy.deepcopy(a4_tmp))\n",
    "        \n",
    "        # the outputs of the 5th layer\n",
    "        activation5 = np.array([np.sum(activation4[l]) for l in range(self.size[-1])])\n",
    "        activation5 = activation5.reshape(-1,1)\n",
    "        return activation1, activation2, activation3, activation4, activation5        \n",
    "\n",
    "    def inference(self,x):\n",
    "        \"\"\"Given a 2-D array and infere the output by using the ANFIS.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a 2-D array, shape(n,1)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        y: a 2-D array, shape (m,1)\n",
    "        \"\"\"\n",
    "        activation1, activation2, \\\n",
    "        activation3, activation4, activation5 = self.forwardpass(x)\n",
    "        return activation5 # output 2-D, shape(m,1)\n",
    "\n",
    "    def total_cost(self, evaluation_data):\n",
    "        \"\"\"Calculate the total cost on the given data set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        evaluation_data: a list of tuples each (x,y)\n",
    "        where x is 1-D array and y is also 1-D array\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        Cost, a real number\n",
    "        \"\"\"\n",
    "        Cost = 0.0\n",
    "        for (x,y) in evaluation_data:\n",
    "            output = self.inference(x.reshape(-1,1))\n",
    "            Cost += (self.cost.C(output,y.reshape(-1,1)))\n",
    "        return Cost/len(evaluation_data)\n",
    "        \n",
    "    def average_accuracy(self,evaluation_data):\n",
    "        \"\"\"Calculate the averaged cost on the given data set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        evaluation_data: a list of tuples each (x,y)\n",
    "        where x is 1-D array and y is also 1-D array\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        avgAcc\n",
    "        \"\"\"\n",
    "        Acc = 0.0\n",
    "        m = len(evaluation_data[0][1])\n",
    "        for (x,y) in evaluation_data:\n",
    "            output = self.inference(x.reshape(-1,1)) #2-D\n",
    "            acc = 0.0\n",
    "            for l in range(m):\n",
    "                ol = output[l,0]\n",
    "                yl = y[l]\n",
    "                if (np.abs(yl) == 0) and (np.abs(ol)==0):\n",
    "                    acc += 1.\n",
    "                elif (np.abs(yl) == 0) and (np.abs(ol)!=0):\n",
    "                    acc += 0.0\n",
    "                else:\n",
    "                    acc += np.abs((yl-ol)/yl)  \n",
    "            acc = acc/m\n",
    "            Acc+=acc            \n",
    "        return Acc/len(evaluation_data)\n",
    "        \n",
    "    def stochastic_gradient_descent(self,training_data0,\n",
    "                                    validation_data0=None,\n",
    "                                    eta = 10.0,\n",
    "                                    mini_batch_size=100,\n",
    "                                    epoch = 50,\n",
    "                                    adapt_eta_mode = True,\n",
    "                                    evaluation_track_flag=False,\n",
    "                                    training_track_flag=False):\n",
    "        \"\"\"Do the SGD algorithm to train the ANFIS.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data: a list of tuples each denoting a sample pair (x,y)\n",
    "        x is a 1-D array in shape (n,), and y is also a 1-D array in shape (m,)\n",
    "        n and m are the numbers of inputs and outputs per sample, respectively.\n",
    "        validation_data: a list of tuples similar to training_data\n",
    "        \n",
    "        eta: the learning rate, a real positive number\n",
    "        mini_batch_size: the size of batched samples\n",
    "        epoch: training epoch number\n",
    "        {evaluation, training}_track_flag: a flag, bool\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        evaluation_cost_trace: a 1-D array recording the cost evaluated on evaluation data\n",
    "        at the end of each epoch, an empty list if its corresponding flag is False\n",
    "        training_cost_trace: a 1-D array recording the cost on the training data,\n",
    "        an empty list if its corresponding flag is False\n",
    "        \"\"\"\n",
    "        training_data = copy.deepcopy(training_data0)\n",
    "        validation_data = copy.deepcopy(validation_data0)\n",
    "        evaluation_cost = []\n",
    "        evaluation_accuracy =[]\n",
    "        training_cost = []\n",
    "        training_accuracy = []\n",
    "        total_sample_num = len(training_data)\n",
    "        track_4points = np.zeros((5,))\n",
    "        t_count = 0\n",
    "        for t in xrange(epoch):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batch_list = [\n",
    "                training_data[j:j+mini_batch_size]\n",
    "                for j in xrange(0,total_sample_num,mini_batch_size)]\n",
    "            for mini_batch in mini_batch_list:\n",
    "                self.update_ANFIS(mini_batch,eta,adapt_eta_mode)\n",
    "#                # adapt the learning rate by using two heuristic rules\n",
    "#                eval_cost_mini_batch = self.total_cost(mini_batch)\n",
    "#                if t_count<5:\n",
    "#                    track_4points[t_count] = eval_cost_mini_batch\n",
    "#                else:\n",
    "#                    for ti in range(4):\n",
    "#                        track_4points[ti] = track_4points[ti+1]\n",
    "#                    track_4points[-1] = eval_cost_mini_batch\n",
    "#                if (track_4points[1]>track_4points[2]) \\\n",
    "#                and (track_4points[2]>track_4points[3]) \\\n",
    "#                and (track_4points[3]>track_4points[4]):\n",
    "#                    eta = eta*(1.1)\n",
    "#                elif (track_4points[0]<track_4points[1]) \\\n",
    "#                and (track_4points[1]>track_4points[2]) \\\n",
    "#                and (track_4points[2]<track_4points[3]) \\\n",
    "#                and (track_4points[3]>track_4points[4]):\n",
    "#                    eta = eta*(0.9)\n",
    "#                t_count += 1\n",
    "                    \n",
    "            print \"#Complete the epoch: {0}/{1}\".format(t,epoch)\n",
    "            if training_track_flag:\n",
    "                eval_cost = self.total_cost(training_data)\n",
    "                eval_acc = self.average_accuracy(training_data)\n",
    "                print \"*Total training cost: {0}\".format(eval_cost)\n",
    "                print \"Averaged training accuracy: {0}\".format(eval_acc)\n",
    "                training_cost.append(eval_cost)\n",
    "                training_accuracy.append(eval_acc)\n",
    "                # adapt the learning rate by using two heuristic rules\n",
    "                eval_cost_mini_batch = eval_cost\n",
    "                if t_count<5:\n",
    "                    track_4points[t_count] = eval_cost_mini_batch\n",
    "                else:\n",
    "                    for ti in range(4):\n",
    "                        track_4points[ti] = track_4points[ti+1]\n",
    "                    track_4points[-1] = eval_cost_mini_batch\n",
    "                if (track_4points[1]>track_4points[2]) \\\n",
    "                and (track_4points[2]>track_4points[3]) \\\n",
    "                and (track_4points[3]>track_4points[4]):\n",
    "                    eta = eta*(1.1)\n",
    "                elif (track_4points[0]<track_4points[1]) \\\n",
    "                and (track_4points[1]>track_4points[2]) \\\n",
    "                and (track_4points[2]<track_4points[3]) \\\n",
    "                and (track_4points[3]>track_4points[4]):\n",
    "                    eta = eta*(0.9)\n",
    "                t_count += 1\n",
    "            if validation_data:\n",
    "                eval_cost = self.total_cost(validation_data)\n",
    "                eval_acc = self.average_accuracy(validation_data)\n",
    "                print \"*Total validation cost: {0}\".format(eval_cost)\n",
    "                print \"Averaged valiation accuracy: {0}\".format(eval_acc)\n",
    "                if evaluation_track_flag:\n",
    "                    evaluation_cost.append(eval_cost)\n",
    "                    evaluation_accuracy.append(eval_acc)\n",
    "            print \" \"   \n",
    "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "     \n",
    "    def update_ANFIS(self,mini_batch, eta, adapt_eta_mode=True):\n",
    "        \"\"\"Update ANFIS\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mini_batch: a list of tuples\n",
    "        eta: a learning rate\n",
    "        \"\"\"\n",
    "        # rearrange the training data into X and Y\n",
    "        mini_batch_size = len(mini_batch)\n",
    "        (x,y) = mini_batch[0]\n",
    "        X = x.reshape(1,-1)\n",
    "        Y = y.reshape(1,-1)\n",
    "        for k in xrange(mini_batch_size):\n",
    "            if k>0:\n",
    "                (x,y) = mini_batch[k]\n",
    "                X = np.vstack((X,x.reshape(1,-1)))\n",
    "                Y = np.vstack((Y,y.reshape(1,-1)))\n",
    "        # do main learning algorithm        \n",
    "        dpremiseParam = self.process_mini_batch(X,Y)\n",
    "        \n",
    "        # adapt the learning rate\n",
    "        if adapt_eta_mode:\n",
    "            den = 0.0\n",
    "            for i in range(self.size[0]):\n",
    "                for ji in range(self.mf_num_vector[i]):\n",
    "                    den += (np.linalg.norm(dpremiseParam[i][ji]))**2\n",
    "            den = np.sqrt(den)\n",
    "            if den<=1e-4:\n",
    "                lmbda = eta\n",
    "            else:\n",
    "                lmbda =eta/den\n",
    "        else:\n",
    "            lmbda = eta\n",
    "        \n",
    "        # update the premise parameters\n",
    "        for i in range(self.size[0]):\n",
    "            for ji in range(self.mf_num_vector[i]):\n",
    "                self.premiseParam[i][ji] = self.premiseParam[i][ji] \\\n",
    "                -(lmbda/mini_batch_size)*dpremiseParam[i][ji]\n",
    "        \n",
    "   \n",
    "    def process_mini_batch(self,mini_batch_X, mini_batch_Y):\n",
    "        \"\"\"Calculate the error rates of all the premise parameters over the\n",
    "        mini_batch data by using backpropagation.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        activation1: a list of lists each containing the membership function \n",
    "        outputs of an input, shape (n,M), where M = m1+m2+...+mn, the total number\n",
    "        of the mfs\n",
    "        \n",
    "        activation2: a list, shape (M,)\n",
    "        \n",
    "        activation3: a list, shape (N,) where N=m1*m2*...*mn, the total rules number\n",
    "        \n",
    "        activation4: the activations of the 4th layer, 2-D list, i.e., a list of lists\n",
    "        each corresponding to an output channel, shape (m, N)\n",
    "        \n",
    "        activation5: a 2-D array containing m outputs, shape (m,1)\n",
    "        \n",
    "        mini_batch_X: a 2-D array, shape (mini_batch_size,n)\n",
    "        mini_batch_Y: a 2-D array, shape (mini_batch_size,m)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dpremiseParameter in the same shape with that of self.premiseParam,\n",
    "        a list of lists each containing the error rates of mfs\n",
    "        \"\"\"\n",
    "        dpremiseParameter = []\n",
    "        for i in range(self.size[0]):\n",
    "            dpremiseParameter.append([np.zeros(self.premiseParam[i][ji].shape) for ji in range(self.mf_num_vector[i])])\n",
    "        \n",
    "        activation1_list = []\n",
    "        activation2_list = []\n",
    "        activation3_list = []\n",
    "        activation4_list = []\n",
    "        activation5_list = []\n",
    "        for x in mini_batch_X:\n",
    "            # forward pass\n",
    "            act1, act2, \\\n",
    "            act3, act4, act5 = self.forwardpass(x.reshape(-1,1)) \n",
    "            #act1, act2, act3 = self.activations_3rd_layer(x.reshape(-1,1))\n",
    "            activation3_list.append(copy.deepcopy(act3))\n",
    "            activation1_list.append(copy.deepcopy(act1))\n",
    "            activation2_list.append(copy.deepcopy(act2))\n",
    "            activation4_list.append(copy.deepcopy(act4))\n",
    "            activation5_list.append(copy.deepcopy(act5))\n",
    "        \n",
    "        # update the consequent parameters with fixed premise parameters\n",
    "        self.batch_update_consequentParam(activation3_list,\n",
    "                                          mini_batch_X,mini_batch_Y) \n",
    "        index = 0                                  \n",
    "        # update the premise parameters with fixed consequent parameters\n",
    "        for x, y in zip(mini_batch_X,mini_batch_Y):\n",
    "            # reshape x and y into 2-D\n",
    "            x = x.reshape(-1,1)\n",
    "            y = y.reshape(-1,1)\n",
    "            \n",
    "            # error rates per sample\n",
    "            dP = self.backpropagation(activation1_list[index], \\\n",
    "                                      activation2_list[index], \\\n",
    "                        activation3_list[index], \\\n",
    "                        activation4_list[index], \\\n",
    "                        activation5_list[index], \\\n",
    "                        x, y)\n",
    "            index += 1\n",
    "            #raise(\"HELLO-0\")\n",
    "            for i in range(self.size[0]):\n",
    "                for ji in range(self.mf_num_vector[i]):\n",
    "                    dpremiseParameter[i][ji] = dpremiseParameter[i][ji] + dP[i][ji]\n",
    "        #show_preParam(self,dpremiseParameter)            \n",
    "        return dpremiseParameter    \n",
    "    \n",
    "    def backpropagation(self, activation1, activation2,\n",
    "                        activation3, activation4, activation5,\n",
    "                        x, y):\n",
    "        \"\"\"Perform the backpropagation algorithm to derive the error rates of\n",
    "        each premise parameters in the 1st layer. a single input-output pair is\n",
    "        given.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        activation1: a list of lists each containing the membership function \n",
    "        outputs of an input, shape (n,M), where M = m1+m2+...+mn, the total number\n",
    "        of the mfs\n",
    "        \n",
    "        activation2: a list, shape (M,)\n",
    "        \n",
    "        activation3: a list, shape (N,) where N=m1*m2*...*mn, the total rules number\n",
    "        \n",
    "        activation4: the activations of the 4th layer, 2-D list, i.e., a list of lists\n",
    "        each corresponding to an output channel, shape (m, N)\n",
    "        \n",
    "        activation5: a 2-D array containing m outputs, shape (m,1)\n",
    "        \n",
    "        x: a 2-D array, shape (n,1)\n",
    "        y: a 2-D array, shape (m,1)\n",
    "        \n",
    "        Returns\n",
    "        -----------        \n",
    "        a list in the same shape with that of activation1\n",
    "        containing the error rates of premise parametrs.\n",
    "        \"\"\"\n",
    "        X = np.vstack((np.array([[1.0]]),x)) #shape (n+1,1)        \n",
    "        \n",
    "        #M = np.sum(self.mf_num_vector)\n",
    "        N = np.prod(self.mf_num_vector)\n",
    "        \n",
    "        # the 5-th layer\n",
    "        delta5 = self.cost.delta(activation5, y) # a 2-D array in shape (m,1)\n",
    "\n",
    "        # the 4-th layer\n",
    "        delta4 = []\n",
    "        for l in range(self.size[-1]):\n",
    "            tmp_delta4 = []\n",
    "            for q in range(N):\n",
    "                tmp_delta4.append(delta5[l,0])\n",
    "            delta4.append(copy.deepcopy(tmp_delta4)) # like activation4\n",
    "        \n",
    "        # the 3rd layer\n",
    "        delta3 = []\n",
    "        for q in range(N):\n",
    "            tmp3 = 0.0\n",
    "            for l in range(self.size[-1]):\n",
    "                w = self.consequentParam[l][q,::]\n",
    "                tmp3 += delta4[l][q]*(np.dot(w.reshape(1,-1),X)[0,0]) # Note that tmp3 is a 2-D array-like variable!!!\n",
    "            delta3.append(tmp3)\n",
    "        delta3 = np.asarray(delta3)\n",
    "\n",
    "        # the 2nd layer\n",
    "        sum_a2 = np.sum(np.exp(activation2))\n",
    "        delta2 = []\n",
    "        for q in range(N):\n",
    "            tmp_da2 = 0.0\n",
    "            for s in range(N):\n",
    "                if s==q:\n",
    "                    tmp_da2 += (delta3[s])*(np.exp(activation2[q])*((sum_a2-np.exp(activation2[q])))/(sum_a2**2))\n",
    "                else:\n",
    "                    tmp_da2 += (delta3[s])*((-np.exp(activation2[s])*np.exp(activation2[q]))/(sum_a2**2))\n",
    "            delta2.append(tmp_da2)\n",
    "\n",
    "        # the 1st layer\n",
    "        delta1 = []\n",
    "        for i in range(self.size[0]):\n",
    "            tmp_delta1 = []\n",
    "            for ji in range(self.mf_num_vector[i]):\n",
    "                inds = get_indices_of_2ndLayer(i,ji,self.indices_2nd_layer)\n",
    "                tmp1 = 0.0\n",
    "                for ind_2nd_layer_node in inds:\n",
    "                    inds_tuple = self.indices_2nd_layer[ind_2nd_layer_node]\n",
    "                    \n",
    "                    partial1_vector = [activation1[i_prime][inds_tuple[i_prime]] \\\n",
    "                    for i_prime in range(self.size[0]) if i_prime != i]\n",
    "                    \n",
    "                    tmp1 += delta2[ind_2nd_layer_node]*(np.prod(partial1_vector))\n",
    "                tmp_delta1.append(tmp1)\n",
    "            delta1.append(copy.deepcopy(tmp_delta1))\n",
    "        # the deltas of premise parameters in the same shape with that of premiseParam\n",
    "        dpremiseParam = []\n",
    "        for i in range(self.size[0]):\n",
    "            dpremiseParam.append([delta1[i][ji]*dgbellmf(x[i,0],self.premiseParam[i][ji]) for ji \\\n",
    "                                                 in range(self.mf_num_vector[i])])\n",
    "        return dpremiseParam\n",
    "\n",
    "    def batch_update_consequentParam(self, activation3_list, mini_batch_X, mini_batch_Y):\n",
    "        \"\"\"Update the consequent parameters by using the least square estimator.\n",
    "        The update is carried out with batch activations.        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        activation3: a list of lists each containing the activations of the 3rd layer at each sample\n",
    "        mini_batch_X: a 2-D array, shape (mini_batch_size,n)\n",
    "        mini_batch_Y: a 2-D array, shape (mini_batch_size,m)\n",
    "        n is the inputs number and m the outputs number.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dpremiseParam: a list of lists each containing the error rates of mfs\n",
    "        its shape is the same to that of self.premiseParam\n",
    "        \"\"\"\n",
    "        (mini_batch_size,n) = mini_batch_X.shape\n",
    "        mini_batch_x = np.hstack((np.ones((mini_batch_size,1), dtype=float),mini_batch_X))\n",
    "        \n",
    "        for l in range(self.size[-1]):\n",
    "            mini_batch_y = mini_batch_Y[::,l] # a 1-D array, shape(mini_batch_size,)\n",
    "            \n",
    "            # update the consequent parameters related to the l-th output\n",
    "            consequentParam = self.consequentParam[l] # obtain the parameters\n",
    "            # vectorize this form\n",
    "            solution = lse.vectorize(consequentParam)\n",
    "            # do lse\n",
    "            solution = lse.LSE3(activation3_list,mini_batch_x,mini_batch_y,solution)\n",
    "            # update the l-th consequent parameters\n",
    "            self.consequentParam[l] = solution.reshape(consequentParam.shape)\n",
    "\n",
    "\n",
    "# Miscellaneous functions\n",
    "def generateFIS(mini_batch,mf_num_vector):\n",
    "    \"\"\"This function creates an initial Sugeno-type FIS\n",
    "    for ANFIS training by performing\n",
    "    a grid partiion on the given data. In this function, \n",
    "    the generalized bell-shaped function,\n",
    "    i.e.,f(x;a,b,c) = 1/(1+((x-c)/a)^(2b)), \n",
    "    is adopted as the membership function of each input, and\n",
    "    a linear weighed sum, i.e. sum_{i=1 to n}{wi*xi}+r, is adopted as the \n",
    "    membership function of a single output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mini_batch: a list of tuples each denoting sample pair (x,y)\n",
    "    mf_num_vector: a list of mf numbers\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    consequentParam: a list of 2-D arrays each with shape (N,n+1) given N rules and n inputs. This\n",
    "    consequentParam is arranged as [[w_mi]], m=0,...,N-1; i=0,..,n\n",
    "    premiseParam: a list of lists each containing the parameter arrays of membership\n",
    "    functions, like [[1-D-array,...,1-D-array],...]\n",
    "    \n",
    "    the parameters set of the i-th node's j-th membership function is indexed by\n",
    "    premiseParam[i][j], which is an array containing a, b, and c\n",
    "    consequentParam[m,i] denotes the weight of the m-th rule's i-th input\n",
    "    \"\"\"   \n",
    "    mini_batch_size = len(mini_batch)\n",
    "    (x,y) = mini_batch[0]\n",
    "    X = x.reshape(1,-1)\n",
    "    Y = y.reshape(1,-1)\n",
    "    for k in xrange(mini_batch_size):\n",
    "        if k>0:\n",
    "            (x,y) = mini_batch[k]\n",
    "            X = np.vstack((X,x.reshape(1,-1)))\n",
    "            Y = np.vstack((Y,y.reshape(1,-1)))\n",
    "    return generateFIS2(X, Y, mf_num_vector)\n",
    "\n",
    "\n",
    "def generateFIS2(X, Y, mf_num_vector):\n",
    "    \"\"\"This function creates an initial Sugeno-type FIS\n",
    "    for ANFIS training by performing\n",
    "    a grid partiion on the given data. In this function, \n",
    "    the generalized bell-shaped function,\n",
    "    i.e.,f(x;a,b,c) = 1/(1+((x-c)/a)^(2b)), \n",
    "    is adopted as the membership function of each input, and\n",
    "    a linear weighed sum, i.e. sum_{i=1 to n}{wi*xi}+r, is adopted as the \n",
    "    membership function of a single output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: a 2-D array, shape (P,n) where P is the sample number and n is the \n",
    "    input number of each sample.\n",
    "    Y: a 2-D array, shape(P,out_num) where P is the sampel number and \n",
    "    out_num is the output number.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    consequentParam: a list of 2-D arrays each with shape (N,n+1) given N rules and n inputs. This\n",
    "    consequentParam is arranged as [[w_mi]], m=0,...,N-1; i=0,..,n\n",
    "    premiseParam: a list of lists each containing the parameter arrays of membership\n",
    "    functions, like [[1-D-array,...,1-D-array],...]\n",
    "    \n",
    "    the parameters set of the i-th node's j-th membership function is indexed by\n",
    "    premiseParam[i][j], which is an array containing a, b, and c\n",
    "    consequentParam[m,i] denotes the weight of the m-th rule's i-th input\n",
    "    \"\"\"\n",
    "    # Generate the initial consequent parameters of the membership function of\n",
    "    # the single output\n",
    "    (P,n) = X.shape\n",
    "    (PP,output_num) = Y.shape\n",
    "    if len(mf_num_vector) == 0:\n",
    "        mf_num_vector = [2 for i in range(n)]\n",
    "        \n",
    "    N = np.prod(mf_num_vector)\n",
    "    consequentParam = [np.ones((N,n+1),dtype=float) for l in range(output_num)]\n",
    "\n",
    "\n",
    "    # Generate the initial premise parameters\n",
    "    premiseParam = []\n",
    "    for i in range(n):\n",
    "        # for the i-th input\n",
    "        vmin = np.min(X[::,i])\n",
    "        vmin = vmin*0.99\n",
    "        vmax = np.max(X[::,i])\n",
    "        vmax = vmax*1.01\n",
    "        if mf_num_vector[i]==1:\n",
    "            raise ValueError(\"The number of membership functions of the %d-th input should not be less than 1!\" %\n",
    "                             i+1)\n",
    "        a = (vmax-vmin)/2/(mf_num_vector[i]-1)\n",
    "        b = 2.0\n",
    "        c_vector = np.linspace(vmin,vmax,num=mf_num_vector[i])\n",
    "        #premiseParam_node_i = [np.array([a,b,c]) for c in c_vector]\n",
    "        premiseParam.append([np.array([a,b,c]) for c in c_vector])\n",
    "    return premiseParam, consequentParam\n",
    "    \n",
    "def index_of_2ndLayer(mf_num_vector):\n",
    "    \"\"\"This function generates the index of each node in the 2nd layer\n",
    "    of the ANFIS.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mf_num_vector: a list like [m1,m2,...,mn] in which mi denotes the number of \n",
    "    membership functions w.r.t. i, i in {1,2,...,n}\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dict_index: a dictionary-type variable \n",
    "    like {(0,0,...,0):0,(0,0,...,1):1,..,(m1-1,m2-1,...,mn-1):N} where (j1,j2,..,jn)\n",
    "    is a key, a tuple of indices of every node's membership functions, given that\n",
    "    ji is the index of node i's j-th membership function and\n",
    "    N = m1*m2*...*mn. Note that ji is ranging from 0 to mi.\n",
    "    \n",
    "    indices: a list of all the tuples each containing a combination of indices\n",
    "    in the first layer, i.e., denoting a key in dict_index\n",
    "    \"\"\"\n",
    "    I = []\n",
    "\n",
    "    for mi in mf_num_vector:\n",
    "        index = [ji for ji in range(mi)]\n",
    "        I.append(index)\n",
    "    indices = []\n",
    "    indices = recursion(indices,0,I)\n",
    "    #return {indices[value]:value for value in range(len(indices))}, indices\n",
    "    return indices\n",
    "    \n",
    "    \n",
    "def recursion(indices,i,I):\n",
    "    \"\"\"This function constructs a list of tuples of indices by using a recursion\n",
    "    mechanism.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    indices: a list of tuples like [(j1,j2,...,ji-1),...,(l1,l2,...,li-1)]\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    indices: a list of all the tuples each containing a combination of indices\n",
    "    \"\"\"\n",
    "    n = len(I)\n",
    "    if len(indices)==0:\n",
    "        indices = [(x,) for x in I[i]]\n",
    "        return recursion(indices,i+1,I)\n",
    "    elif i == (n-1):\n",
    "        return [inds+(x,) for inds in indices for x in I[i]]\n",
    "    else:\n",
    "        indices = [inds+(x,) for inds in indices for x in I[i]]\n",
    "        return recursion(indices,i+1,I)\n",
    "        \n",
    "\n",
    "def get_indices_of_2ndLayer(i,j,indices):\n",
    "    \"\"\"Given the index ji of a membership function in the 1st layer, indicated by (i,j),\n",
    "    get the indices of the nodes in the 2nd layer that are connecting to this \n",
    "    membership function node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    i: an integer, ranging from 0 to n when considering there are n inputs\n",
    "    j: an integer, ranging from 0 to mi where mi is the number of membership\n",
    "    functions associated with the i-th input\n",
    "    indices: a list of tuples each containing a combination of indices of all\n",
    "    the membership functions\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    m: a list containing the indices of the nodes in the 2nd layer that are \n",
    "    connecting to the i-th node's j-th membership function.\n",
    "    \"\"\"\n",
    "    m = []\n",
    "    for ind in range(len(indices)):\n",
    "        key = indices[ind]\n",
    "        if key[i] == j:\n",
    "            m.append(ind)\n",
    "    return m\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "    from prepare_data import load_test_data\n",
    "    training_data,validation_data,all_data = load_test_data()\n",
    "    # network initialization\n",
    "    net = ANFIS([4,1])\n",
    "    net.initialization(training_data)\n",
    "    # train the net\n",
    "    evaluation_cost, evaluation_accuracy, \\\n",
    "    training_cost, training_accuracy =net.stochastic_gradient_descent(training_data,\n",
    "                                    validation_data,\n",
    "                                    eta = 0.1,\n",
    "                                    mini_batch_size=10,\n",
    "                                    epoch = 30,\n",
    "                                    adapt_eta_mode = True,\n",
    "                                    evaluation_track_flag=True,\n",
    "                                    training_track_flag=True)\n",
    "    \n",
    "    predictions = np.array([net.inference(x.reshape(-1,1))[0,0] for \\\n",
    "    (x,y) in validation_data])\n",
    "    actual_outputs = np.array([y[0] for (x,y) in validation_data])\n",
    "    labelfont = {\"family\":\"serif\",\"size\":25}\n",
    "    plt.figure(0,figsize=(9,8))\n",
    "    plt.plot(actual_outputs,'-b',lw=8.0, label=\"Actual\")\n",
    "    plt.plot(predictions,'--r',lw=8.0, label=\"ANFIS\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(prop={\"size\":labelfont[\"size\"],\"family\":\"serif\"})  \n",
    "    xlabelstr = \"Epoch $k$\"\n",
    "    ylabelstr = \"Fuel level\"\n",
    "    plt.xlabel(xlabelstr,fontdict=labelfont)\n",
    "    plt.ylabel(ylabelstr,fontdict=labelfont)\n",
    "    plt.xticks(fontsize=labelfont[\"size\"],\\\n",
    "    fontname=labelfont[\"family\"])\n",
    "    plt.yticks(fontsize=labelfont[\"size\"],\n",
    "               family=labelfont[\"family\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demonstration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Copyright (C) Thu Dec 01 11:07:31 2016  Jianshan Zhou\n",
    "Contact: zhoujianshan@buaa.edu.cn\tjianshanzhou@foxmail.com\n",
    "Website: <https://github.com/JianshanZhou>\n",
    "\n",
    "This program is free software: you can redistribute\n",
    " it and/or modify it under the terms of\n",
    " the GNU General Public License as published\n",
    " by the Free Software Foundation,\n",
    " either version 3 of the License,\n",
    " or (at your option) any later version.\n",
    " \n",
    "This program is distributed in the hope that it will be useful,\n",
    " but WITHOUT ANY WARRANTY;\n",
    " without even the implied warranty of MERCHANTABILITY\n",
    " or FITNESS FOR A PARTICULAR PURPOSE.\n",
    " See the GNU General Public License for more details.\n",
    " You should have received a copy of the GNU General Public License\n",
    " along with this program.\n",
    " If not, see <http://www.gnu.org/licenses/>.\n",
    " \n",
    "This module carries out some experiments where the ANFIS will be applied\n",
    "to identify an unknown system given a time series of the system outputs,\n",
    "and to achieve adaptive nonlinear noise cancellation.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from prepare_data import load_test_data1\n",
    "from anfis import ANFIS\n",
    "import numpy as np\n",
    "import copy\n",
    "from anfis_mfs import gbellmf\n",
    "    \n",
    "def experiment1(training_data,validation_data,all_data):    \n",
    "    #training_data,validation_data,all_data = load_test_data1()\n",
    "    # network initialization\n",
    "    net = ANFIS([4,1])\n",
    "    net.initialization(training_data)\n",
    "    init_net = copy.deepcopy(net)\n",
    "    # train the net\n",
    "    evaluation_cost, evaluation_accuracy, \\\n",
    "    training_cost, training_accuracy =net.stochastic_gradient_descent(training_data,\n",
    "                                    validation_data,\n",
    "                                    eta = 0.1,\n",
    "                                    mini_batch_size=50,\n",
    "                                    epoch = 25,\n",
    "                                    adapt_eta_mode = True,\n",
    "                                    evaluation_track_flag=True,\n",
    "                                    training_track_flag=True)\n",
    "\n",
    "    return evaluation_cost, evaluation_accuracy, \\\n",
    "    training_cost, training_accuracy, net, init_net\n",
    "\n",
    "def visulization1(evaluation_cost, evaluation_accuracy, \\\n",
    "    training_cost, training_accuracy, init_net, net,\\\n",
    "    training_data,validation_data):\n",
    "    lc = ['r', 'g', 'b', 'y']\n",
    "    ls = ['-', '--', ':', '-.']    \n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rc(\"font\",family=\"serif\")\n",
    "    \n",
    "    # Show the initial mfs\n",
    "    mini_batch_size = len(training_data)\n",
    "    (x,y) = training_data[0]\n",
    "    input_num = len(x)\n",
    "    X = x.reshape(1,-1)\n",
    "    Y = y.reshape(1,-1)\n",
    "    for k in xrange(mini_batch_size):\n",
    "        if k>0:\n",
    "            (x,y) = training_data[k]\n",
    "            X = np.vstack((X,x.reshape(1,-1)))\n",
    "            Y = np.vstack((Y,y.reshape(1,-1)))\n",
    "            \n",
    "    labelfont = {\"family\":\"serif\",\"size\":25}\n",
    "    plt.figure(3,figsize=(10,8))\n",
    "    plt.suptitle(r\"Initial membership functions of $\\displaystyle\\mathbf{X}$\",\\\n",
    "    fontsize=labelfont[\"size\"])\n",
    "    for i in range(input_num):\n",
    "        # for the i-th input\n",
    "        vmin = np.min(X[::,i])\n",
    "        vmin = vmin*0.99\n",
    "        vmax = np.max(X[::,i])\n",
    "        vmax = vmax*1.01\n",
    "        input_xi = np.linspace(vmin,vmax,500)\n",
    "        # the i-th input\n",
    "        activation1_list = []\n",
    "        for ji in range(init_net.mf_num_vector[i]):\n",
    "            params = init_net.premiseParam[i][ji]\n",
    "            activation1_list.append(np.array([gbellmf(xi,params) for xi in input_xi]))\n",
    "        plt.subplot(4,1,i+1)\n",
    "        plt.grid(True)\n",
    "        for ji in range(init_net.mf_num_vector[i]):\n",
    "            plt.plot(input_xi,activation1_list[ji],lc[ji]+ls[ji],lw=6.)\n",
    "        plt.xlabel(r\"Input: x{0}\".format(i+1))\n",
    "        plt.ylabel(r\"Outputs of MFs\")\n",
    "        plt.legend(loc=0)\n",
    "    plt.subplots_adjust(hspace=0.39)\n",
    "        \n",
    "    plt.figure(4,figsize=(10,8))\n",
    "    plt.grid(True)\n",
    "    plt.suptitle(r\"Final membership functions of $\\displaystyle\\mathbf{X}$\",\\\n",
    "    fontsize=labelfont[\"size\"])\n",
    "    for i in range(input_num):\n",
    "        # for the i-th input\n",
    "        vmin = np.min(X[::,i])\n",
    "        vmin = vmin*0.99\n",
    "        vmax = np.max(X[::,i])\n",
    "        vmax = vmax*1.01\n",
    "        input_xi = np.linspace(vmin,vmax,500)\n",
    "        # the i-th input\n",
    "        activation1_list = []\n",
    "        for ji in range(net.mf_num_vector[i]):\n",
    "            params = net.premiseParam[i][ji]\n",
    "            activation1_list.append(np.array([gbellmf(xi,params) for xi in input_xi]))\n",
    "        plt.subplot(4,1,i+1)\n",
    "        plt.grid(True)\n",
    "        for ji in range(net.mf_num_vector[i]):\n",
    "            plt.plot(input_xi,activation1_list[ji],lc[ji]+ls[ji],lw=6.)\n",
    "        plt.xlabel(r\"Input: x{0}\".format(i+1))\n",
    "        plt.ylabel(r\"Outputs of MFs\")\n",
    "        plt.legend(loc=0)\n",
    "    plt.subplots_adjust(hspace=0.39)\n",
    "        \n",
    "    # Show the training cost and the validation cost\n",
    "    labelfont = {\"family\":\"serif\",\"size\":25}\n",
    "    plt.figure(0,figsize=(10,8))\n",
    "    \n",
    "    plt.plot(training_cost,'-ob',lw=8.0, ms=16, label=\"Training cost\")\n",
    "    plt.plot(evaluation_cost,'--or',lw=8.0, ms=16, label=\"Validation cost\")\n",
    "    plt.grid(True)\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.subplots_adjust(left=0.18)\n",
    "    plt.legend(prop={\"size\":labelfont[\"size\"],\"family\":\"serif\"},loc=0)  \n",
    "    xlabelstr = r\"Time epoch $t$\"\n",
    "    plt.title(r\"ANFIS cost: $\\displaystyle\\frac{1}{P}\\sum_{p=1}^{P}{\\Vert\"\n",
    "    r\"\\mathbf{a}_{p}^{5}-\\mathbf{y}_{p}\\Vert_2^2}$\",fontdict=labelfont)\n",
    "    plt.xlabel(xlabelstr,fontdict=labelfont)\n",
    "    plt.ylabel(r\"Total cost\",fontdict=labelfont)\n",
    "    plt.xticks(fontsize=labelfont[\"size\"],\\\n",
    "    fontname=labelfont[\"family\"])\n",
    "    plt.yticks(fontsize=labelfont[\"size\"],\n",
    "               family=labelfont[\"family\"])\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the training cost and the validation cost\n",
    "    labelfont = {\"family\":\"serif\",\"size\":25}\n",
    "    plt.figure(1,figsize=(10,8))\n",
    "    plt.plot(np.asarray(training_accuracy)*100,'-ob',lw=8.0, ms=16, label=\"Training accuracy\")\n",
    "    plt.plot(np.asarray(evaluation_accuracy)*100,'--or',lw=8.0, ms=16, label=\"Validation accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.subplots_adjust(left=0.15)\n",
    "    plt.legend(prop={\"size\":labelfont[\"size\"],\"family\":\"serif\"},loc=0)  \n",
    "    xlabelstr = r\"Time epoch $t$\"\n",
    "    plt.title(r\"ANFIS accuracy: \"\n",
    "    r\"$\\displaystyle\\frac{1}{P}\"\n",
    "    r\"\\sum_{p=1}^{P}{\\sum_{l=1}^{m}{\\left\\vert\\frac{a_{l,p}^{5}\"\n",
    "    r\"-y_{l,p}}{y_{l,p}}\\right\\vert}}$\",fontdict=labelfont)\n",
    "    plt.xlabel(xlabelstr,fontdict=labelfont)\n",
    "    plt.ylabel(r\"Relative error [%]\",fontdict=labelfont)\n",
    "    plt.xticks(fontsize=labelfont[\"size\"],\\\n",
    "    fontname=labelfont[\"family\"])\n",
    "    plt.yticks(fontsize=labelfont[\"size\"],\n",
    "               family=labelfont[\"family\"])\n",
    "    plt.show()\n",
    "    \n",
    "    # validation\n",
    "    predictions = np.array([net.inference(x.reshape(-1,1))[0,0] for \\\n",
    "    (x,y) in validation_data])\n",
    "    actual_outputs = np.array([y[0] for (x,y) in validation_data])\n",
    "    labelfont = {\"family\":\"serif\",\"size\":25}\n",
    "    plt.figure(2,figsize=(10,8))\n",
    "    plt.subplots_adjust(left=0.1)\n",
    "    plt.plot(actual_outputs,'-b',lw=8.0, ms=16, label=\"Actual\")\n",
    "    plt.plot(predictions,'--r',lw=8.0, ms=16, label=\"ANFIS\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(prop={\"size\":labelfont[\"size\"],\\\n",
    "    \"family\":\"serif\"},ncol=2,loc=0)  \n",
    "    xlabelstr = r\"Time epoch $t$\"\n",
    "    ylabelstr = r\"System output\"\n",
    "    plt.title(\"Model validation\",fontdict=labelfont)\n",
    "    plt.xlabel(xlabelstr,fontdict=labelfont)\n",
    "    plt.ylabel(ylabelstr,fontdict=labelfont)\n",
    "    plt.xticks(fontsize=labelfont[\"size\"],\\\n",
    "    fontname=labelfont[\"family\"])\n",
    "    plt.yticks(fontsize=labelfont[\"size\"],\n",
    "               family=labelfont[\"family\"])\n",
    "    plt.show()\n",
    "\n",
    "def carry_out_experiment1():\n",
    "    training_data,validation_data,all_data = load_test_data1()\n",
    "    evaluation_cost, evaluation_accuracy, \\\n",
    "        training_cost, training_accuracy, \\\n",
    "        net, init_net=experiment1(training_data,validation_data,all_data)\n",
    "    visulization1(evaluation_cost, evaluation_accuracy, \\\n",
    "        training_cost, training_accuracy, init_net, net,\\\n",
    "        training_data,validation_data)\n",
    "\n",
    "      \n",
    "if __name__ == \"__main__\":\n",
    "    carry_out_experiment1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
